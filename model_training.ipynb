{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ChorismaAI: Training a Chord Recognition Model\n",
    "\n",
    "## Introduction to Project Scenario\n",
    "**Problem Statement**\n",
    "\n",
    "As avid guitarists, Liam and myself constantly love to experiment with new guitar tunings and chords. However, there are moments when we may play a chord without explicitly knowing its name in the process of music creation. For alike musicians, this imposes a strict barrier on using music theory knowledge to build on nicely sounding chords and \n",
    "\n",
    "**Proposed Solution**\n",
    "\n",
    "Our team consisting of members Dillon de Silva, Liam Ling, Lachlan Liu and Bernard Tam has designed the app *Chorisma*, which aims to provides users with a simple and intuitive way to leverage an accurate and expandable ML model for chord recognition. Not only do we provide a scalable, ML solution but we also aim to provide the maximum benefit to musicians creating music, making our tool an essential for production.\n",
    "\n",
    "## Model Training\n",
    "\n",
    "ChorismaAI takes raw time-domain signal recordings as user input and aims to return a categorical label, annotating what chord is recognized in the recording. At a high level, we constructed our model by extracting useful features from this signal and taking classical approaches to developing a high accuracy model\n",
    "\n",
    "### Feature Extraction\n",
    "\n",
    "Chords are comprised of several musical notes (typically $\\ge$ 3 notes). Each note corresponds to a musical pitch and therefore, some vibrational frequency. Also, time-domain signals alone are not inherently useful for performing chord recognition. \n",
    "\n",
    "With all these constraints and facts in mind, we require the use of a Fourier Transform to be able to analyze the spectral densities of frequencies present in the signal. Using this, we can obtain the chromagram of our signal which provides us with the spectral amplitudes of the 12 musical pitches. Since different chords will have varying intensities of each pitch, we can use this as a means of classification for our model.\n",
    "\n",
    "Note: Short-Time Fourier Transform (STFT) was used to ensure our model handles performing fourier transforms on windowed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Installing dependencies for feature extraction\n",
    "%pip install pandas librosa numpy tqdm --quiet\n",
    "\n",
    "# Import modules\n",
    "from tqdm import tqdm\n",
    "\n",
    "import librosa\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "# Configuring path to training data\n",
    "TRAINING_PATH = \"Data/Training\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting chromagram from raw audio file\n",
    "def get_chromagram(raw_audio_path):\n",
    "    raw_audio_ts, sr = librosa.load(raw_audio_path)\n",
    "    chromagram = np.array(librosa.feature.chroma_stft(y=raw_audio_ts, sr=sr, center=True), dtype=object)\n",
    "    chromagram = np.mean(chromagram, axis=1) # Calculating the mean spectral intensity across each of 12 pitches\n",
    "    return chromagram\n",
    "\n",
    "# Gets name of chord based on subdirectory folder name\n",
    "def get_chord_from_subdir(subdir):\n",
    "    subdir_components = subdir.split(\"/\")\n",
    "    return subdir_components[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9it [00:29,  3.28s/it]\n"
     ]
    }
   ],
   "source": [
    "# Aggregating our chromagram to chord feature set\n",
    "chromagram_to_chord_data = []\n",
    "\n",
    "# Loop through chord directories in test data\n",
    "for subdir, dirs, files in tqdm(os.walk(TRAINING_PATH)):\n",
    "    chord_name = get_chord_from_subdir(subdir)\n",
    "    for file in files:\n",
    "        raw_audio_path = os.path.join(subdir, file)\n",
    "        chromagram = get_chromagram(raw_audio_path)\n",
    "        feature_data = [chord_name] \n",
    "        for pitch in chromagram:\n",
    "            feature_data.append(pitch) # Add each pitch as its own element in our feature data\n",
    "\n",
    "        chromagram_to_chord_data.append(feature_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Am</td>\n",
       "      <td>0.544287</td>\n",
       "      <td>0.150015</td>\n",
       "      <td>0.062872</td>\n",
       "      <td>0.291705</td>\n",
       "      <td>0.578690</td>\n",
       "      <td>0.165433</td>\n",
       "      <td>0.073326</td>\n",
       "      <td>0.089554</td>\n",
       "      <td>0.139484</td>\n",
       "      <td>0.314026</td>\n",
       "      <td>0.457331</td>\n",
       "      <td>0.546481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Am</td>\n",
       "      <td>0.477938</td>\n",
       "      <td>0.159540</td>\n",
       "      <td>0.078124</td>\n",
       "      <td>0.217118</td>\n",
       "      <td>0.757103</td>\n",
       "      <td>0.235772</td>\n",
       "      <td>0.070104</td>\n",
       "      <td>0.206644</td>\n",
       "      <td>0.270980</td>\n",
       "      <td>0.513716</td>\n",
       "      <td>0.444653</td>\n",
       "      <td>0.706167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Am</td>\n",
       "      <td>0.297920</td>\n",
       "      <td>0.117693</td>\n",
       "      <td>0.047192</td>\n",
       "      <td>0.235275</td>\n",
       "      <td>0.913052</td>\n",
       "      <td>0.269611</td>\n",
       "      <td>0.079305</td>\n",
       "      <td>0.302352</td>\n",
       "      <td>0.216573</td>\n",
       "      <td>0.346569</td>\n",
       "      <td>0.235025</td>\n",
       "      <td>0.291966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Am</td>\n",
       "      <td>0.343310</td>\n",
       "      <td>0.127324</td>\n",
       "      <td>0.049306</td>\n",
       "      <td>0.214362</td>\n",
       "      <td>0.883645</td>\n",
       "      <td>0.234823</td>\n",
       "      <td>0.086475</td>\n",
       "      <td>0.299380</td>\n",
       "      <td>0.235693</td>\n",
       "      <td>0.430943</td>\n",
       "      <td>0.284206</td>\n",
       "      <td>0.359154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Am</td>\n",
       "      <td>0.416165</td>\n",
       "      <td>0.251908</td>\n",
       "      <td>0.265860</td>\n",
       "      <td>0.153626</td>\n",
       "      <td>0.819963</td>\n",
       "      <td>0.738397</td>\n",
       "      <td>0.083050</td>\n",
       "      <td>0.099816</td>\n",
       "      <td>0.180761</td>\n",
       "      <td>0.394568</td>\n",
       "      <td>0.429308</td>\n",
       "      <td>0.353965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Am</td>\n",
       "      <td>0.086091</td>\n",
       "      <td>0.148181</td>\n",
       "      <td>0.082484</td>\n",
       "      <td>0.203551</td>\n",
       "      <td>0.734694</td>\n",
       "      <td>0.381427</td>\n",
       "      <td>0.066975</td>\n",
       "      <td>0.080422</td>\n",
       "      <td>0.164592</td>\n",
       "      <td>0.461862</td>\n",
       "      <td>0.269435</td>\n",
       "      <td>0.110513</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0         1         2         3         4         5         6         7   \\\n",
       "0  Am  0.544287  0.150015  0.062872  0.291705  0.578690  0.165433  0.073326   \n",
       "1  Am  0.477938  0.159540  0.078124  0.217118  0.757103  0.235772  0.070104   \n",
       "2  Am  0.297920  0.117693  0.047192  0.235275  0.913052  0.269611  0.079305   \n",
       "3  Am  0.343310  0.127324  0.049306  0.214362  0.883645  0.234823  0.086475   \n",
       "4  Am  0.416165  0.251908  0.265860  0.153626  0.819963  0.738397  0.083050   \n",
       "5  Am  0.086091  0.148181  0.082484  0.203551  0.734694  0.381427  0.066975   \n",
       "\n",
       "         8         9         10        11        12  \n",
       "0  0.089554  0.139484  0.314026  0.457331  0.546481  \n",
       "1  0.206644  0.270980  0.513716  0.444653  0.706167  \n",
       "2  0.302352  0.216573  0.346569  0.235025  0.291966  \n",
       "3  0.299380  0.235693  0.430943  0.284206  0.359154  \n",
       "4  0.099816  0.180761  0.394568  0.429308  0.353965  \n",
       "5  0.080422  0.164592  0.461862  0.269435  0.110513  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Exporting to pickled object/csv for training\n",
    "chromagram_to_chord_df = pd.DataFrame(data=chromagram_to_chord_data)\n",
    "chromagram_to_chord_df.to_csv('TrainingData.csv')\n",
    "chromagram_to_chord_df.to_pickle('TrainingData.pkl')\n",
    "\n",
    "chromagram_to_chord_df.head(n=6)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial Model Architecture Choice: Random Forest Classifier\n",
    "\n",
    "Currently, only Random Forest Classifiers are used by ChorismaAI due to its simplicity in implementation and reliable performance with trivial classification problems using annotated datasets. However in future, the use of ASTs could be interesting as these are the state of the art technology in Audio ML Tasks.\n",
    "\n",
    "Training was performed using the pickled training data object from feature extraction. Additionally, we used the default hyperparameters for a RandomForestClassifier provided by Scikit-Learn on an 80/20 training-test split (Pareto Distribution)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Installing dependencies for model training\n",
    "%pip install sklearn --quiet\n",
    "\n",
    "# Import modules\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestClassifier(random_state=44)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier(random_state=44)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomForestClassifier(random_state=44)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initializing random forest classifier (with default hyperparameters) \n",
    "# and performing model training\n",
    "rf_clf = RandomForestClassifier(\n",
    "    random_state=44\n",
    ")\n",
    "\n",
    "train_data = pd.read_pickle('TrainingData.pkl').reset_index(drop=True)\n",
    "\n",
    "Y = train_data.iloc[0:, 0]\n",
    "X = train_data.iloc[0:, 1:]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "\n",
    "rf_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R-Squared Score on Test Split: 0.96875\n"
     ]
    }
   ],
   "source": [
    "r2_score = rf_clf.score(X_test, y_test)\n",
    "print(f\"R-Squared Score on Test Split: {r2_score}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our reported $r^2$ accuracy is $\\approx 97\\%$. Whilst this is a satisfactory result, let's also write an interactive set of code blocks to allow us to experiment with our own guitar recordings and \"spot-check\" if the model is working as expected. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- MODIFY THIS ---\n",
    "MY_CHORD_PATH = 'Data/Experimental/Random_Bb_Chord.wav' # Put the path to your recorded chord file here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PREDICTED CHORD: ['Bb']\n"
     ]
    }
   ],
   "source": [
    "# -- DO NOT CHANGE THIS ---\n",
    "# Run this block to find predicted chord based on your file\n",
    "my_chord = get_chromagram(MY_CHORD_PATH).reshape(1, -1)\n",
    "pred = rf_clf.predict(my_chord)\n",
    "print(f\"PREDICTED CHORD: {pred}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
